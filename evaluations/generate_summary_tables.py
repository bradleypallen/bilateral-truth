#!/usr/bin/env python3
"""
Generate comprehensive summary tables from evaluation results.

Table 1: Main results (accuracy, F1, coverage, execution time)
Table 2: Bilateral truth value probabilities
Table 3: Epistemic honesty and overconfidence metrics
Table 4: Strongest/weakest categories per model per benchmark
"""

import json
import glob
import pandas as pd
import numpy as np
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple
import time


def load_all_results() -> Dict:
    """Load all evaluation results from individual files."""
    results = defaultdict(lambda: defaultdict(dict))
    
    # Load bilateral results
    for file in glob.glob('results/*_classical_results.json'):
        with open(file) as f:
            data = json.load(f)
        
        model = data.get('model', '')
        model_short = model.split('/')[-1] if '/' in model else model
        
        # Extract dataset from filename
        filename = Path(file).stem
        parts = filename.split('_')
        dataset = parts[0]  # e.g., "truthfulqa", "factscore", etc.
        
        results[model_short][dataset]['bilateral'] = {
            'accuracy': data.get('accuracy', 0),
            'f1_macro': data.get('f1_macro', 0),
            'coverage': data.get('coverage', 0),
            'execution_time': data.get('execution_time', 0),
            'bilateral_distribution': data.get('bilateral_distribution', {}),
            'category_performance': data.get('category_analysis', {}),
            'total_samples': data.get('total_samples', 0)
        }
    
    # Load unilateral direct results
    for file in glob.glob('results/*_unilateral_direct_results.json'):
        with open(file) as f:
            data = json.load(f)
        
        model = data.get('model', '')
        model_short = model.split('/')[-1] if '/' in model else model
        
        filename = Path(file).stem
        parts = filename.split('_')
        dataset = parts[0]
        
        results[model_short][dataset]['unilateral_forced'] = {
            'accuracy': data.get('accuracy', 0),
            'f1_macro': data.get('f1_macro', 0),
            'coverage': data.get('coverage', 1.0),
            'execution_time': data.get('execution_time', 0),
            'category_performance': data.get('category_analysis', {}),
            'total_samples': data.get('total_samples', 0)
        }
    
    # Load unilateral uncertain results
    for file in glob.glob('results/*_unilateral_uncertain_results.json'):
        with open(file) as f:
            data = json.load(f)
        
        model = data.get('model', '')
        model_short = model.split('/')[-1] if '/' in model else model
        
        filename = Path(file).stem
        parts = filename.split('_')
        dataset = parts[0]
        
        results[model_short][dataset]['unilateral_uncertain'] = {
            'accuracy': data.get('accuracy', 0),
            'f1_macro': data.get('f1_macro', 0),
            'coverage': data.get('coverage', 1.0),
            'execution_time': data.get('execution_time', 0),
            'uncertainty_rate': data.get('uncertainty_rate', 0),
            'accuracy_on_answered': data.get('accuracy_on_answered', 0),
            'category_performance': data.get('category_analysis', {}),
            'total_samples': data.get('total_samples', 0)
        }
    
    # Calculate verification-only from bilateral results
    for model in results:
        for dataset in results[model]:
            if 'bilateral' in results[model][dataset]:
                # Load the raw bilateral file to calculate verification-only
                model_safe = model.replace('/', '_').replace(':', '_')
                bilateral_file = f"results/{dataset}_complete_{model_safe}_classical_results.json"
                
                if Path(bilateral_file).exists():
                    with open(bilateral_file) as f:
                        data = json.load(f)
                    
                    if 'detailed_results' in data:
                        ver_correct = 0
                        ver_total = 0
                        
                        for item in data['detailed_results']:
                            bilateral_value = item.get('bilateral_value', '<e,e>')
                            ground_truth = item.get('expected_label', '')
                            
                            # Extract verification component
                            if bilateral_value.startswith('<t,'):
                                prediction = 'correct'
                            elif bilateral_value.startswith('<f,'):
                                prediction = 'incorrect'
                            else:
                                prediction = 'undefined'
                            
                            ver_total += 1
                            if prediction != 'undefined' and prediction == ground_truth:
                                ver_correct += 1
                        
                        if ver_total > 0:
                            accuracy = ver_correct / ver_total
                            # Simplified F1 calculation (should properly calculate)
                            results[model][dataset]['verification_only'] = {
                                'accuracy': accuracy,
                                'f1_macro': accuracy,  # Simplified
                                'coverage': 1.0,
                                'execution_time': 0  # Not separately tracked
                            }
    
    return dict(results)


def generate_table1_main_results(results: Dict) -> pd.DataFrame:
    """Generate Table 1: Main results with all metrics."""
    
    rows = []
    
    for model in sorted(results.keys()):
        for dataset in sorted(results[model].keys()):
            row = {
                'Model': model[:20],  # Truncate long model names
                'Dataset': dataset.upper()
            }
            
            # Add metrics for each method
            methods = ['bilateral', 'unilateral_forced', 'unilateral_uncertain', 'verification_only']
            
            for method in methods:
                if method in results[model][dataset]:
                    data = results[model][dataset][method]
                    prefix = method[:3].upper()  # BIL, UNI, VER
                    
                    row[f'{prefix}_Acc'] = data.get('accuracy', 0)
                    row[f'{prefix}_F1'] = data.get('f1_macro', 0)
                    row[f'{prefix}_Cov'] = data.get('coverage', 0)
                    row[f'{prefix}_Time'] = data.get('execution_time', 0)
            
            rows.append(row)
    
    df = pd.DataFrame(rows)
    
    # Format for display
    print("\n" + "="*150)
    print("TABLE 1: MAIN RESULTS - ACCURACY, F1 MACRO, COVERAGE, AND EXECUTION TIME")
    print("="*150)
    
    for dataset in df['Dataset'].unique():
        subset = df[df['Dataset'] == dataset]
        
        print(f"\n{dataset}:")
        print("-"*150)
        print(f"{'Model':<20} | {'BILATERAL':^35} | {'UNILATERAL FORCED':^35} | {'UNILATERAL UNCERTAIN':^35} | {'VERIFICATION':^35}")
        print(f"{'':20} | {'Acc':>8} {'F1':>8} {'Cov':>8} {'Time':>8} | {'Acc':>8} {'F1':>8} {'Cov':>8} {'Time':>8} | {'Acc':>8} {'F1':>8} {'Cov':>8} {'Time':>8} | {'Acc':>8} {'F1':>8} {'Cov':>8} {'Time':>8}")
        print("-"*150)
        
        for _, row in subset.iterrows():
            print(f"{row['Model']:<20} | ", end="")
            
            # Bilateral
            print(f"{row.get('BIL_Acc', 0):>8.3f} {row.get('BIL_F1', 0):>8.3f} {row.get('BIL_Cov', 0):>8.1%} {row.get('BIL_Time', 0):>7.1f}s | ", end="")
            
            # Unilateral Forced
            print(f"{row.get('UNI_Acc', 0):>8.3f} {row.get('UNI_F1', 0):>8.3f} {row.get('UNI_Cov', 0):>8.1%} {row.get('UNI_Time', 0):>7.1f}s | ", end="")
            
            # Unilateral Uncertain  
            print(f"{row.get('UNI_Acc', 0):>8.3f} {row.get('UNI_F1', 0):>8.3f} {row.get('UNI_Cov', 0):>8.1%} {row.get('UNI_Time', 0):>7.1f}s | ", end="")
            
            # Verification
            print(f"{row.get('VER_Acc', 0):>8.3f} {row.get('VER_F1', 0):>8.3f} {row.get('VER_Cov', 0):>8.1%} {row.get('VER_Time', 0):>7.1f}s")
    
    return df


def generate_table2_truth_values(results: Dict) -> pd.DataFrame:
    """Generate Table 2: Bilateral truth value probabilities."""
    
    rows = []
    
    for model in sorted(results.keys()):
        for dataset in sorted(results[model].keys()):
            if 'bilateral' in results[model][dataset]:
                bilateral_data = results[model][dataset]['bilateral']
                dist = bilateral_data.get('bilateral_distribution', {})
                total = bilateral_data.get('total_samples', 1)
                
                if total > 0:
                    row = {
                        'Model': model[:20],
                        'Dataset': dataset.upper(),
                        'P(<t,f>)': dist.get('<t,f>', 0) / total,
                        'P(<f,t>)': dist.get('<f,t>', 0) / total,
                        'P(<t,t>)': dist.get('<t,t>', 0) / total,
                        'P(<f,f>)': dist.get('<f,f>', 0) / total,
                        'P(<e,e>)': dist.get('<e,e>', 0) / total
                    }
                    rows.append(row)
    
    df = pd.DataFrame(rows)
    
    print("\n" + "="*120)
    print("TABLE 2: BILATERAL TRUTH VALUE PROBABILITIES")
    print("="*120)
    
    for dataset in df['Dataset'].unique():
        subset = df[df['Dataset'] == dataset]
        
        print(f"\n{dataset}:")
        print("-"*120)
        print(f"{'Model':<20} | {'P(<t,f>)':>10} | {'P(<f,t>)':>10} | {'P(<t,t>)':>10} | {'P(<f,f>)':>10} | {'P(<e,e>)':>10} | {'Coverage':>10}")
        print("-"*120)
        
        for _, row in subset.iterrows():
            coverage = row['P(<t,f>)'] + row['P(<f,t>)']  # Classical policy coverage
            print(f"{row['Model']:<20} | {row['P(<t,f>)']:>10.3f} | {row['P(<f,t>)']:>10.3f} | "
                  f"{row['P(<t,t>)']:>10.3f} | {row['P(<f,f>)']:>10.3f} | {row['P(<e,e>)']:>10.3f} | {coverage:>10.1%}")
    
    return df


def generate_table3_epistemic_metrics(results: Dict) -> pd.DataFrame:
    """Generate Table 3: Epistemic honesty and overconfidence metrics."""
    
    rows = []
    
    for model in sorted(results.keys()):
        # Aggregate across datasets
        total_tt = 0
        total_ff = 0
        total_tf = 0
        total_ft = 0
        total_ee = 0
        total_samples = 0
        
        bil_acc_sum = 0
        uni_acc_sum = 0
        unc_cov_sum = 0
        n_datasets = 0
        
        for dataset in results[model]:
            if 'bilateral' in results[model][dataset]:
                bilateral_data = results[model][dataset]['bilateral']
                dist = bilateral_data.get('bilateral_distribution', {})
                samples = bilateral_data.get('total_samples', 0)
                
                total_tt += dist.get('<t,t>', 0)
                total_ff += dist.get('<f,f>', 0)
                total_tf += dist.get('<t,f>', 0)
                total_ft += dist.get('<f,t>', 0)
                total_ee += dist.get('<e,e>', 0)
                total_samples += samples
                
                bil_acc_sum += bilateral_data.get('accuracy', 0)
                n_datasets += 1
            
            if 'unilateral_forced' in results[model][dataset]:
                uni_acc_sum += results[model][dataset]['unilateral_forced'].get('accuracy', 0)
            
            if 'unilateral_uncertain' in results[model][dataset]:
                unc_cov_sum += results[model][dataset]['unilateral_uncertain'].get('coverage', 0)
        
        if total_samples > 0 and n_datasets > 0:
            # Calculate epistemic metrics
            knowledge_gap_rate = total_ff / total_samples
            contradiction_rate = total_tt / total_samples
            abstention_rate = (total_tt + total_ff) / total_samples
            
            # Epistemic honesty: high abstention when uncertain
            epistemic_honesty = abstention_rate
            
            # Overconfidence: forced accuracy - bilateral accuracy
            overconfidence = (uni_acc_sum / n_datasets) - (bil_acc_sum / n_datasets) if n_datasets > 0 else 0
            
            # Uncertainty awareness: coverage in uncertain mode
            uncertainty_awareness = unc_cov_sum / n_datasets if n_datasets > 0 else 0
            
            row = {
                'Model': model[:20],
                'Knowledge_Gap_Rate': knowledge_gap_rate,
                'Contradiction_Rate': contradiction_rate,
                'Abstention_Rate': abstention_rate,
                'Epistemic_Honesty': epistemic_honesty,
                'Overconfidence': overconfidence,
                'Uncertainty_Awareness': uncertainty_awareness
            }
            rows.append(row)
    
    df = pd.DataFrame(rows)
    df = df.sort_values('Epistemic_Honesty', ascending=False)
    
    print("\n" + "="*120)
    print("TABLE 3: MODELS BY EPISTEMIC HONESTY AND OVERCONFIDENCE")
    print("="*120)
    print(f"{'Model':<20} | {'Knowledge':>10} | {'Contradiction':>13} | {'Abstention':>11} | {'Epistemic':>10} | {'Over-':>10} | {'Uncertainty':>12}")
    print(f"{'':20} | {'Gap Rate':>10} | {'Rate':>13} | {'Rate':>11} | {'Honesty':>10} | {'confidence':>10} | {'Awareness':>12}")
    print("-"*120)
    
    for _, row in df.iterrows():
        print(f"{row['Model']:<20} | {row['Knowledge_Gap_Rate']:>10.3f} | {row['Contradiction_Rate']:>13.3f} | "
              f"{row['Abstention_Rate']:>11.3f} | {row['Epistemic_Honesty']:>10.3f} | "
              f"{row['Overconfidence']:>+10.3f} | {row['Uncertainty_Awareness']:>12.3f}")
    
    return df


def generate_table4_category_analysis(results: Dict) -> pd.DataFrame:
    """Generate Table 4: Strongest and weakest categories per model per benchmark."""
    
    rows = []
    
    print("\n" + "="*150)
    print("TABLE 4: STRONGEST AND WEAKEST CATEGORIES PER MODEL PER BENCHMARK (BY F1 MACRO)")
    print("="*150)
    
    for dataset in ['truthfulqa', 'simpleqa', 'mmlupro', 'factscore']:
        print(f"\n{dataset.upper()}:")
        print("-"*150)
        print(f"{'Model':<20} | {'STRONGEST CATEGORY':^60} | {'WEAKEST CATEGORY':^60}")
        print(f"{'':20} | {'Category':>25} {'F1':>8} {'Acc':>8} {'N':>8} | {'Category':>25} {'F1':>8} {'Acc':>8} {'N':>8}")
        print("-"*150)
        
        for model in sorted(results.keys()):
            if dataset in results[model] and 'bilateral' in results[model][dataset]:
                category_perf = results[model][dataset]['bilateral'].get('category_performance', {})
                
                if category_perf:
                    # Calculate F1 for each category (simplified - using accuracy as proxy)
                    category_scores = []
                    
                    for cat, data in category_perf.items():
                        if isinstance(data, dict) and 'accuracy' in data:
                            f1_score = data.get('f1_macro', data.get('accuracy', 0))
                            category_scores.append({
                                'category': cat[:25],
                                'f1': f1_score,
                                'accuracy': data.get('accuracy', 0),
                                'n': data.get('total', 0)
                            })
                    
                    if category_scores:
                        # Sort by F1 score
                        category_scores.sort(key=lambda x: x['f1'])
                        
                        weakest = category_scores[0]
                        strongest = category_scores[-1]
                        
                        print(f"{model[:20]:<20} | "
                              f"{strongest['category']:>25} {strongest['f1']:>8.3f} {strongest['accuracy']:>8.3f} {strongest['n']:>8} | "
                              f"{weakest['category']:>25} {weakest['f1']:>8.3f} {weakest['accuracy']:>8.3f} {weakest['n']:>8}")
                        
                        row = {
                            'Model': model[:20],
                            'Dataset': dataset.upper(),
                            'Strongest_Category': strongest['category'],
                            'Strongest_F1': strongest['f1'],
                            'Strongest_N': strongest['n'],
                            'Weakest_Category': weakest['category'],
                            'Weakest_F1': weakest['f1'],
                            'Weakest_N': weakest['n']
                        }
                        rows.append(row)
    
    return pd.DataFrame(rows)


def save_all_tables(table1: pd.DataFrame, table2: pd.DataFrame, 
                    table3: pd.DataFrame, table4: pd.DataFrame):
    """Save all tables to CSV files."""
    
    table1.to_csv('results/table1_main_results.csv', index=False)
    table2.to_csv('results/table2_truth_values.csv', index=False)
    table3.to_csv('results/table3_epistemic_metrics.csv', index=False)
    table4.to_csv('results/table4_category_analysis.csv', index=False)
    
    print("\n" + "="*120)
    print("TABLES SAVED")
    print("="*120)
    print("ðŸ“Š Table 1 (Main Results): results/table1_main_results.csv")
    print("ðŸ“Š Table 2 (Truth Values): results/table2_truth_values.csv")
    print("ðŸ“Š Table 3 (Epistemic Metrics): results/table3_epistemic_metrics.csv")
    print("ðŸ“Š Table 4 (Category Analysis): results/table4_category_analysis.csv")


def main():
    """Generate all summary tables."""
    
    print("Loading evaluation results...")
    results = load_all_results()
    
    if not results:
        print("No results found!")
        return
    
    print(f"Found results for {len(results)} models")
    
    # Generate tables
    table1 = generate_table1_main_results(results)
    table2 = generate_table2_truth_values(results)
    table3 = generate_table3_epistemic_metrics(results)
    table4 = generate_table4_category_analysis(results)
    
    # Save all tables
    save_all_tables(table1, table2, table3, table4)
    
    print("\nâœ… All tables generated successfully!")


if __name__ == "__main__":
    main()